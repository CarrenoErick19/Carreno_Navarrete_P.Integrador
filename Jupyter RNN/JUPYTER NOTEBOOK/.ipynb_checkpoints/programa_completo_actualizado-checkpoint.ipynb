{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "225b9038",
   "metadata": {},
   "source": [
    "# SECCIÓN 1: ANÁLISIS DE SENTIMIENTOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5416279c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Inicializar el analizador de sentimientos de VADER\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Diccionario de emociones para clasificar\n",
    "sentimientos = {\n",
    "    'alegría': 0, \n",
    "    'enojo': 1, \n",
    "    'tristeza': 2, \n",
    "    'satisfacción': 3, \n",
    "    'insatisfacción': 4\n",
    "}\n",
    "\n",
    "# Palabras clave y frases específicas del dialecto ecuatoriano para emociones negativas\n",
    "palabras_clave_negativas = [\n",
    "    'sinvergüenza', 'charlatán', 'mañoso', 'corrupto', 'inútil', 'maldito', 'ladrón', 'hipócrita',\n",
    "    'vergonzoso', 'mentiroso', 'desgraciado', 'canalla', 'ratero'\n",
    "]\n",
    "\n",
    "# Función para clasificar el sentimiento basado en el análisis de VADER y palabras clave específicas\n",
    "def clasificar_sentimiento(comentario):\n",
    "    analisis = TextBlob(comentario)\n",
    "    vader_result = analyzer.polarity_scores(comentario)\n",
    "    \n",
    "    # Clasificación basada en el análisis de VADER\n",
    "    if vader_result['compound'] >= 0.5:\n",
    "        return 'alegría'\n",
    "    elif vader_result['compound'] >= 0.1:\n",
    "        return 'satisfacción'\n",
    "    elif vader_result['compound'] <= -0.5:\n",
    "        return 'enojo'\n",
    "    elif vader_result['compound'] <= -0.1:\n",
    "        return 'tristeza'\n",
    "    \n",
    "    # Clasificación adicional basada en palabras clave específicas\n",
    "    for palabra in palabras_clave_negativas:\n",
    "        if palabra in comentario:\n",
    "            return 'enojo'\n",
    "    \n",
    "    return 'insatisfacción'\n",
    "\n",
    "def analizar_sentimientos(df):\n",
    "    emociones = {'alegría': [], 'enojo': [], 'tristeza': [], 'satisfacción': [], 'insatisfacción': []}\n",
    "    \n",
    "    aspectos = []\n",
    "    sentim = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        comentario = row['comment_limpio']\n",
    "        sentimiento = clasificar_sentimiento(comentario)\n",
    "        emociones[sentimiento].append(comentario)\n",
    "        aspectos.append('positivo' if sentimiento in ['alegría', 'satisfacción'] else 'negativo' if sentimiento in ['enojo', 'tristeza'] else 'neutral')\n",
    "        sentim.append(sentimiento)\n",
    "    \n",
    "    df['aspecto'] = aspectos\n",
    "    df['sentimiento'] = sentim\n",
    "    df['sentimiento_label'] = df['sentimiento'].map(sentimientos)\n",
    "    \n",
    "    return df, emociones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72acfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def comparar_aspectos(df):\n",
    "    # Implementación de la función para comparar aspectos\n",
    "    pass  # Reemplaza con la lógica real\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f3b578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def identificar_temas(df):\n",
    "    # Aquí va la lógica para identificar temas\n",
    "    # Por ahora, simplemente devolvamos una lista vacía como ejemplo\n",
    "    temas = []\n",
    "    return temas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719c652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def generar_matriz_confusion(y_true, y_pred, labels, ax):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels, ax=ax)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('True')\n",
    "    ax.set_title('Confusion Matrix')\n",
    "\n",
    "def generar_visualizaciones(df, y_true, y_pred, labels):\n",
    "    # Mapeo de los sentimientos para etiquetas comprensibles\n",
    "    sentimiento_map = {0: 'alegría', 1: 'enojo', 2: 'tristeza', 3: 'satisfacción', 4: 'insatisfacción'}\n",
    "    df['sentimiento_label'] = df['sentimiento'].map(sentimiento_map)\n",
    "\n",
    "    # Crear figura con subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Análisis de Sentimientos y Aspectos', fontsize=16)\n",
    "\n",
    "    # Conteo de sentimientos\n",
    "    sns.countplot(x='sentimiento_label', data=df, palette='viridis', ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('Distribución de Sentimientos')\n",
    "    axes[0, 0].set_xlabel('Sentimientos')\n",
    "    axes[0, 0].set_ylabel('Conteo')\n",
    "\n",
    "    # Evolución temporal de los sentimientos\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df.resample('M', on='timestamp').size().plot(ax=axes[0, 1])\n",
    "    axes[0, 1].set_title('Número de Comentarios por Mes')\n",
    "    axes[0, 1].set_xlabel('Fecha')\n",
    "    axes[0, 1].set_ylabel('Número de Comentarios')\n",
    "\n",
    "    # Análisis de temas y aspectos\n",
    "    if 'aspecto' in df.columns:\n",
    "        sns.countplot(x='aspecto', data=df, palette='coolwarm', ax=axes[1, 0])\n",
    "        axes[1, 0].set_title('Distribución de Aspectos')\n",
    "        axes[1, 0].set_xlabel('Aspectos')\n",
    "        axes[1, 0].set_ylabel('Conteo')\n",
    "\n",
    "    # Matriz de confusión\n",
    "    generar_matriz_confusion(y_true, y_pred, labels, axes[1, 1])\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b1d004",
   "metadata": {},
   "source": [
    "# SECCIÓN 2: LIMPIEZA DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8613013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import os\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Descargar y cargar manualmente los recursos necesarios de NLTK si no se han descargado previamente\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "def limpiar_caracteres(texto):\n",
    "    if isinstance(texto, str):\n",
    "        texto_limpio = re.sub(r'[^a-zA-Z\\s]', '', texto, flags=re.I|re.A)\n",
    "        return texto_limpio.lower()\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def tokenizar_texto(texto):\n",
    "    tokens = word_tokenize(texto)\n",
    "    return tokens\n",
    "\n",
    "def eliminar_stopwords(tokens):\n",
    "    stopwords_esp = set(stopwords.words('spanish'))\n",
    "    tokens_filtrados = [token for token in tokens if token.lower() not in stopwords_esp]\n",
    "    return tokens_filtrados\n",
    "\n",
    "def lematizar_tokens(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens_lemmatizados = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return tokens_lemmatizados\n",
    "\n",
    "def limpiar_datos_fila(fila):\n",
    "    fila['comment_limpio'] = limpiar_caracteres(fila['comment'])\n",
    "    tokens = tokenizar_texto(fila['comment_limpio'])\n",
    "    tokens = eliminar_stopwords(tokens)\n",
    "    tokens = lematizar_tokens(tokens)\n",
    "    fila['tokens'] = tokens\n",
    "    return fila\n",
    "\n",
    "def cargar_csv(file_name):\n",
    "    csv_path = os.path.join('datasets', file_name)\n",
    "    df = pd.read_csv(csv_path, delimiter=',')\n",
    "    df.rename(columns={'comment': 'comment'}, inplace=True)  # Asegurarse de que la columna se llama 'comment'\n",
    "    return df\n",
    "\n",
    "def limpiar_datos(df):\n",
    "    filas_limpias = []\n",
    "    for _, fila in df.iterrows():\n",
    "        filas_limpias.append(limpiar_datos_fila(fila))\n",
    "    df_limpio = pd.DataFrame(filas_limpias)\n",
    "    return df_limpio\n",
    "\n",
    "def vectorizar_texto(df):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(df['comment_limpio'])\n",
    "    return tfidf_matrix, tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "def mostrar_resultados(df, tfidf_matrix, feature_names):\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Resultados de Limpieza de Datos\")\n",
    "\n",
    "    tree = ttk.Treeview(root)\n",
    "    tree[\"columns\"] = (\"comment\", \"comment_limpio\", \"tokens\", \"tfidf\")\n",
    "\n",
    "    tree.column(\"#0\", width=0, stretch=tk.NO)\n",
    "    tree.column(\"comment\", anchor=tk.W, width=200)\n",
    "    tree.column(\"comment_limpio\", anchor=tk.W, width=200)\n",
    "    tree.column(\"tokens\", anchor=tk.W, width=200)\n",
    "    tree.column(\"tfidf\", anchor=tk.W, width=200)\n",
    "\n",
    "    tree.heading(\"#0\", text=\"\", anchor=tk.W)\n",
    "    tree.heading(\"comment\", text=\"Comentarios Originales\", anchor=tk.W)\n",
    "    tree.heading(\"comment_limpio\", text=\"Comentarios Limpios\", anchor=tk.W)\n",
    "    tree.heading(\"tokens\", text=\"Tokens\", anchor=tk.W)\n",
    "    tree.heading(\"tfidf\", text=\"TF-IDF\", anchor=tk.W)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        tfidf_vector = tfidf_matrix[index].toarray().flatten()\n",
    "        tfidf_scores = {feature_names[i]: tfidf_vector[i] for i in range(len(feature_names)) if tfidf_vector[i] > 0}\n",
    "        tree.insert(\"\", index, text=\"\", values=(row[\"comment\"], row[\"comment_limpio\"], row[\"tokens\"], str(tfidf_scores)))\n",
    "\n",
    "    tree.pack(expand=True, fill='both')\n",
    "    \n",
    "    root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd775646",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def dividir_datos(df, test_size=0.2, validation_size=0.1):\n",
    "    train_df, test_df = train_test_split(df, test_size=test_size, random_state=42)\n",
    "    train_df, val_df = train_test_split(train_df, test_size=validation_size/(1-test_size), random_state=42)\n",
    "    return train_df, val_df, test_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f6d93c",
   "metadata": {},
   "source": [
    "# SECCIÓN 3: MODELO RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514db21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def definir_modelo_rnn(vocab_size, max_len):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=128, input_length=max_len),\n",
    "        tf.keras.layers.LSTM(64, return_sequences=True),  # Reducir unidades LSTM\n",
    "        tf.keras.layers.LSTM(64),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),  # Reducir unidades Dense\n",
    "        tf.keras.layers.Dense(5, activation='softmax')  # 5 clases para los sentimientos\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b1bda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "def entrenar_y_guardar_modelo_rnn(model, X_train, y_train, X_val, y_val, tokenizer, epochs=500, batch_size=128):\n",
    "    # Implementar Early Stopping\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    \n",
    "    # Compilar el modelo\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Entrenar el modelo con una barra de progreso\n",
    "    with tqdm(total=epochs, desc=\"Training Model\") as pbar:\n",
    "        for epoch in range(epochs):\n",
    "            history = model.fit(X_train, y_train, epochs=1, batch_size=batch_size, validation_data=(X_val, y_val), verbose=0)\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix(loss=history.history['loss'][-1], accuracy=history.history['accuracy'][-1], val_loss=history.history['val_loss'][-1], val_accuracy=history.history['val_accuracy'][-1])\n",
    "            if early_stopping.stopped_epoch > 0:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    # Guardar el modelo y el tokenizer\n",
    "    print(\"Guardando el modelo y el tokenizer...\")\n",
    "    model.save('modelo_rnn.keras')\n",
    "    with open('tokenizer.pickle', 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle)\n",
    "    print(\"Modelo guardado en modelo_rnn.keras.\")\n",
    "    print(\"Tokenizer guardado en tokenizer.pickle.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9fa908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "def evaluar_modelo_rnn(model, test_df, tokenizer):\n",
    "    # Convertir los datos de prueba en secuencias\n",
    "    X_test = tokenizer.texts_to_sequences(test_df['comment_limpio'].values)\n",
    "    X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=50)  # Usar la misma longitud que en el entrenamiento\n",
    "\n",
    "    # Obtener las etiquetas de los datos de prueba\n",
    "    y_test = test_df['sentimiento'].astype('float32')\n",
    "\n",
    "    # Realizar predicciones\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    # Generar la matriz de confusión y el informe de clasificación\n",
    "    cm = confusion_matrix(y_test, y_pred_classes)\n",
    "    cr = classification_report(y_test, y_pred_classes, target_names=['alegría', 'enojo', 'tristeza', 'satisfacción', 'insatisfacción'])\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(cr)\n",
    "\n",
    "    # Visualizar la matriz de confusión\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['alegría', 'enojo', 'tristeza', 'satisfacción', 'insatisfacción'], yticklabels=['alegría', 'enojo', 'tristeza', 'satisfacción', 'insatisfacción'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    return cr, cm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7986a9",
   "metadata": {},
   "source": [
    "# SECCIÓN 4: EJECUCIÓN PRINCIPAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103738c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "from data.data_cleaning import limpiar_datos, vectorizar_texto, mostrar_resultados\n",
    "from analysis.analisis_sentimiento import analizar_sentimientos\n",
    "from rnn_model.definir_modelo import definir_modelo_rnn\n",
    "from rnn_model.evaluar_modelo import evaluar_modelo_rnn\n",
    "from analysis.visualization import generar_visualizaciones\n",
    "from rnn_model.entrenar_modelo import entrenar_y_guardar_modelo_rnn\n",
    "\n",
    "# Cargar datos\n",
    "print(\"Cargando datos...\")\n",
    "df = pd.read_csv('datasets/datos_combinados_1.csv')\n",
    "print(f\"Datos cargados: {len(df)} registros, {len(df.columns)} columnas\")\n",
    "print(df.head(10))\n",
    "\n",
    "# Limpiar datos\n",
    "print(\"Limpiando datos...\")\n",
    "df = limpiar_datos(df)\n",
    "print(f\"Columnas disponibles después de limpiar los datos: {df.columns}\")\n",
    "print(df.head(10))\n",
    "\n",
    "# Vectorizar texto y mostrar resultados\n",
    "print(\"Vectorizando texto...\")\n",
    "tfidf_matrix, feature_names = vectorizar_texto(df)\n",
    "print(\"Mostrando resultados de limpieza de datos...\")\n",
    "mostrar_resultados(df, tfidf_matrix, feature_names)\n",
    "\n",
    "# Análisis de sentimientos\n",
    "print(\"Análisis de sentimientos...\")\n",
    "df, emociones = analizar_sentimientos(df)\n",
    "print(f\"Columnas disponibles después del análisis de sentimientos: {df.columns}\")\n",
    "print(f\"Emociones detectadas: { {key: len(value) for key, value in emociones.items()} }\")\n",
    "print(df.head(10))\n",
    "\n",
    "# Verificar contenido de 'sentimiento_label'\n",
    "print(\"Contenido de 'sentimiento_label':\")\n",
    "print(df['sentimiento_label'].unique())\n",
    "\n",
    "# Corregir mapeo: asegurar que 'sentimiento_label' contenga cadenas\n",
    "sentimiento_map = {'alegría': 0, 'enojo': 1, 'tristeza': 2, 'satisfacción': 3, 'insatisfacción': 4}\n",
    "label_to_text = {v: k for k, v in sentimiento_map.items()}\n",
    "df['sentimiento_label'] = df['sentimiento_label'].map(label_to_text)\n",
    "\n",
    "# Convertir las etiquetas de sentimiento a valores numéricos\n",
    "df['sentimiento'] = df['sentimiento_label'].map(sentimiento_map)\n",
    "\n",
    "# Verificar mapeo\n",
    "print(\"Verificación del mapeo de 'sentimiento_label' a 'sentimiento':\")\n",
    "print(df[['sentimiento_label', 'sentimiento']].drop_duplicates())\n",
    "\n",
    "# Verificar valores NaN antes de eliminar\n",
    "print(f\"Valores NaN en 'sentimiento': {df['sentimiento'].isna().sum()}\")\n",
    "\n",
    "# Manejar valores NaN\n",
    "df = df.dropna(subset=['sentimiento', 'comment_limpio'])\n",
    "\n",
    "# Verificar DataFrame después de eliminar valores NaN\n",
    "print(f\"Datos después de eliminar NaN en 'sentimiento': {len(df)} registros\")\n",
    "print(df.head(10))\n",
    "\n",
    "# Dividir datos en entrenamiento, validación y prueba\n",
    "print(\"Dividiendo datos en entrenamiento, validación y prueba...\")\n",
    "train_df, val_df, test_df = np.split(df.sample(frac=1, random_state=42), [int(.7*len(df)), int(.8*len(df))])\n",
    "print(f\"Datos de entrenamiento: {len(train_df)} registros\")\n",
    "print(f\"Datos de validación: {len(val_df)} registros\")\n",
    "print(f\"Datos de prueba: {len(test_df)} registros\")\n",
    "print(train_df.head(10))\n",
    "\n",
    "# Verificar datos de entrenamiento\n",
    "if len(train_df) == 0:\n",
    "    raise ValueError(\"Datos de entrenamiento están vacíos.\")\n",
    "\n",
    "# Tokenizar los datos\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=5000)  # Reducir el vocabulario a 5000 palabras\n",
    "tokenizer.fit_on_texts(train_df['comment_limpio'].values)\n",
    "X_train = tokenizer.texts_to_sequences(train_df['comment_limpio'].values)\n",
    "X_val = tokenizer.texts_to_sequences(val_df['comment_limpio'].values)\n",
    "X_test = tokenizer.texts_to_sequences(test_df['comment_limpio'].values)  # Tokenizar datos de prueba\n",
    "\n",
    "# Padding\n",
    "max_len = 50  # Reducir el maxlen a 50\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "X_val = tf.keras.preprocessing.sequence.pad_sequences(X_val, maxlen=max_len)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_len)  # Padding de datos de prueba\n",
    "\n",
    "# Convertir etiquetas a numpy arrays\n",
    "y_train = train_df['sentimiento'].values\n",
    "y_val = val_df['sentimiento'].values\n",
    "y_test = test_df['sentimiento'].values  # Etiquetas de prueba\n",
    "\n",
    "# Definir el modelo con una red más simple\n",
    "print(\"Definiendo el modelo...\")\n",
    "model = definir_modelo_rnn(vocab_size=5000, max_len=max_len)\n",
    "\n",
    "# Entrenar el modelo, guardar el modelo y el tokenizer\n",
    "print(\"Entrenando el modelo...\")\n",
    "entrenar_y_guardar_modelo_rnn(model, X_train, y_train, X_val, y_val, tokenizer, epochs=500, batch_size=256)\n",
    "\n",
    "# Evaluar el modelo\n",
    "print(\"Evaluando el modelo...\")\n",
    "cr, cm = evaluar_modelo_rnn(model, test_df, tokenizer)\n",
    "\n",
    "# Generar visualizaciones\n",
    "print(\"Generando visualizaciones...\")\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "generar_visualizaciones(df, y_test, y_pred_classes, ['alegría', 'enojo', 'tristeza', 'satisfacción', 'insatisfacción'])\n",
    "print(\"Visualizaciones generadas.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
