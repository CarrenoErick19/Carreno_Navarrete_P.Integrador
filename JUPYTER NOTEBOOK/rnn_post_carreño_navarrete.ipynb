{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95fb92de",
   "metadata": {},
   "source": [
    "# SECCIÓN 1: ANÁLISIS DE SENTIMIENTOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aad0f128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Inicializar el analizador de sentimientos de VADER\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Diccionario de emociones para clasificar\n",
    "sentimientos = {\n",
    "    'positivo': 0, \n",
    "    'negativo': 1, \n",
    "    'neutral': 2\n",
    "}\n",
    "\n",
    "# Palabras clave y frases específicas del dialecto ecuatoriano para emociones negativas\n",
    "palabras_clave_negativas = [\n",
    "    'sinvergüenza', 'charlatán', 'mañoso', 'corrupto', 'inútil', 'maldito', 'ladrón', 'hipócrita',\n",
    "    'vergonzoso', 'mentiroso', 'desgraciado', 'canalla', 'ratero'\n",
    "]\n",
    "\n",
    "# Función para clasificar el sentimiento basado en el análisis de VADER y palabras clave específicas\n",
    "def clasificar_sentimiento(comentario):\n",
    "    analisis = TextBlob(comentario)\n",
    "    vader_result = analyzer.polarity_scores(comentario)\n",
    "    \n",
    "    # Clasificación basada en el análisis de VADER\n",
    "    if vader_result['compound'] >= 0.05:\n",
    "        return 'positivo'\n",
    "    elif vader_result['compound'] <= -0.05:\n",
    "        return 'negativo'\n",
    "    \n",
    "    # Clasificación adicional basada en palabras clave específicas\n",
    "    for palabra in palabras_clave_negativas:\n",
    "        if palabra in comentario:\n",
    "            return 'negativo'\n",
    "    \n",
    "    return 'neutral'\n",
    "\n",
    "def analizar_sentimientos(df):\n",
    "    emociones = {'positivo': [], 'negativo': [], 'neutral': []}\n",
    "    \n",
    "    aspectos = []\n",
    "    sentim = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        comentario = row['comment_limpio']\n",
    "        sentimiento = clasificar_sentimiento(comentario)\n",
    "        emociones[sentimiento].append(comentario)\n",
    "        aspectos.append(sentimiento)\n",
    "        sentim.append(sentimientos[sentimiento])\n",
    "    \n",
    "    df['aspecto'] = aspectos\n",
    "    df['sentimiento'] = sentim  # Asegúrate de que esta columna se cree correctamente\n",
    "    return df, emociones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a4c791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def comparar_aspectos(df):\n",
    "    # Implementación de la función para comparar aspectos\n",
    "    pass  # Reemplaza con la lógica real\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5146e39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def identificar_temas(df):\n",
    "    # Aquí va la lógica para identificar temas\n",
    "    # Por ahora, simplemente devolvamos una lista vacía como ejemplo\n",
    "    temas = []\n",
    "    return temas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb8e8186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def generar_matriz_confusion(y_true, y_pred, labels, ax):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels, ax=ax)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('True')\n",
    "    ax.set_title('Confusion Matrix')\n",
    "\n",
    "def generar_visualizaciones(df, y_true, y_pred, labels):\n",
    "    # Mapeo de los sentimientos para etiquetas comprensibles\n",
    "    sentimiento_map = {0: 'alegría', 1: 'enojo', 2: 'tristeza', 3: 'satisfacción', 4: 'insatisfacción'}\n",
    "    df['sentimiento_label'] = df['sentimiento'].map(sentimiento_map)\n",
    "\n",
    "    # Crear figura con subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Análisis de Sentimientos y Aspectos', fontsize=16)\n",
    "\n",
    "    # Conteo de sentimientos\n",
    "    sns.countplot(x='sentimiento_label', data=df, palette='viridis', ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('Distribución de Sentimientos')\n",
    "    axes[0, 0].set_xlabel('Sentimientos')\n",
    "    axes[0, 0].set_ylabel('Conteo')\n",
    "\n",
    "    # Evolución temporal de los sentimientos\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df.resample('M', on='timestamp').size().plot(ax=axes[0, 1])\n",
    "    axes[0, 1].set_title('Número de Comentarios por Mes')\n",
    "    axes[0, 1].set_xlabel('Fecha')\n",
    "    axes[0, 1].set_ylabel('Número de Comentarios')\n",
    "\n",
    "    # Análisis de temas y aspectos\n",
    "    if 'aspecto' in df.columns:\n",
    "        sns.countplot(x='aspecto', data=df, palette='coolwarm', ax=axes[1, 0])\n",
    "        axes[1, 0].set_title('Distribución de Aspectos')\n",
    "        axes[1, 0].set_xlabel('Aspectos')\n",
    "        axes[1, 0].set_ylabel('Conteo')\n",
    "\n",
    "    # Matriz de confusión\n",
    "    generar_matriz_confusion(y_true, y_pred, labels, axes[1, 1])\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f24724",
   "metadata": {},
   "source": [
    "# SECCIÓN 2: LIMPIEZA DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f401ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Descargar y cargar manualmente los recursos necesarios de NLTK si no se han descargado previamente\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "def limpiar_caracteres(texto):\n",
    "    if isinstance(texto, str):\n",
    "        texto_limpio = re.sub(r'[^a-zA-Z\\s]', '', texto, flags=re.I|re.A)\n",
    "        return texto_limpio.lower()\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def tokenizar_texto(texto):\n",
    "    tokens = word_tokenize(texto)\n",
    "    return tokens\n",
    "\n",
    "def eliminar_stopwords(tokens):\n",
    "    stopwords_esp = set(stopwords.words('spanish'))\n",
    "    tokens_filtrados = [token for token in tokens if token.lower() not in stopwords_esp]\n",
    "    return tokens_filtrados\n",
    "\n",
    "def lematizar_tokens(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens_lemmatizados = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return tokens_lemmatizados\n",
    "\n",
    "def limpiar_datos_fila(fila):\n",
    "    fila['comment_limpio'] = limpiar_caracteres(fila['comment'])\n",
    "    tokens = tokenizar_texto(fila['comment_limpio'])\n",
    "    tokens = eliminar_stopwords(tokens)\n",
    "    tokens = lematizar_tokens(tokens)\n",
    "    fila['tokens'] = tokens\n",
    "    fila['comment_limpio'] = ' '.join(tokens)  # Asegurarse de que 'comment_limpio' contenga los tokens filtrados\n",
    "    return fila\n",
    "\n",
    "def cargar_csv(file_name):\n",
    "    csv_path = os.path.join('datasets', 'datos_combinados_1.csv')\n",
    "    df = pd.read_csv(csv_path, delimiter=',')\n",
    "    df.rename(columns={'comment': 'comment'}, inplace=True)  # Asegurarse de que la columna se llama 'comment'\n",
    "    return df\n",
    "\n",
    "def limpiar_datos(df):\n",
    "    filas_limpias = []\n",
    "    for _, fila in df.iterrows():\n",
    "        filas_limpias.append(limpiar_datos_fila(fila))\n",
    "    df_limpio = pd.DataFrame(filas_limpias)\n",
    "    return df_limpio\n",
    "\n",
    "def vectorizar_texto(df):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(df['comment_limpio'])\n",
    "    return tfidf_matrix, tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "def mostrar_resultados(df, tfidf_matrix, feature_names):\n",
    "    from collections import Counter\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    all_words = ' '.join(df['comment_limpio']).split()\n",
    "    word_freq = Counter(all_words)\n",
    "    common_words = word_freq.most_common(20)\n",
    "    words, counts = zip(*common_words)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(words, counts)\n",
    "    plt.title('Palabras Más Frecuentes')\n",
    "    plt.xlabel('Palabras')\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14e2972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def dividir_datos(df, test_size=0.2, validation_size=0.1):\n",
    "    train_df, test_df = train_test_split(df, test_size=test_size, random_state=42)\n",
    "    train_df, val_df = train_test_split(train_df, test_size=validation_size/(1-test_size), random_state=42)\n",
    "    return train_df, val_df, test_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5592edec",
   "metadata": {},
   "source": [
    "# SECCIÓN 3: MODELO RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36e25ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def definir_modelo_rnn(vocab_size, max_len):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=max_len))\n",
    "    model.add(Bidirectional(LSTM(units=128, return_sequences=True)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Bidirectional(LSTM(units=64)))\n",
    "    model.add(Dense(units=3, activation='softmax'))  # 3 unidades para las 3 clases de sentimientos\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def entrenar_y_guardar_modelo_rnn(model, X_train, y_train, X_val, y_val, tokenizer, epochs=50, batch_size=32):\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "    model.save('modelo_rnn.h5')\n",
    "    with open('tokenizer.pickle', 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return history\n",
    "\n",
    "def cargar_modelo_rnn():\n",
    "    if os.path.exists('modelo_rnn.h5') and os.path.exists('tokenizer.pickle'):\n",
    "        model = load_model('modelo_rnn.h5')\n",
    "        with open('tokenizer.pickle', 'rb') as handle:\n",
    "            tokenizer = pickle.load(handle)\n",
    "        return model, tokenizer\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "def evaluar_modelo_rnn(model, test_df, tokenizer):\n",
    "    X_test = tokenizer.texts_to_sequences(test_df['comment_limpio'].values)\n",
    "    X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=50)\n",
    "    y_test = test_df['sentimiento'].values\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    cr = classification_report(y_test, y_pred_classes, target_names=['positivo', 'negativo', 'neutral'])\n",
    "    cm = confusion_matrix(y_test, y_pred_classes)\n",
    "    return cr, cm\n",
    "\n",
    "def mostrar_resultados(df, tfidf_matrix, feature_names):\n",
    "    import tkinter as tk\n",
    "    from tkinter import ttk\n",
    "\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Resultados de Limpieza de Datos\")\n",
    "\n",
    "    tree = ttk.Treeview(root)\n",
    "    tree[\"columns\"] = (\"comment\", \"comment_limpio\", \"tokens\", \"tfidf\")\n",
    "\n",
    "    tree.column(\"#0\", width=0, stretch=tk.NO)\n",
    "    tree.column(\"comment\", anchor=tk.W, width=200)\n",
    "    tree.column(\"comment_limpio\", anchor=tk.W, width=200)\n",
    "    tree.column(\"tokens\", anchor=tk.W, width=200)\n",
    "    tree.column(\"tfidf\", anchor=tk.W, width=200)\n",
    "\n",
    "    tree.heading(\"#0\", text=\"\", anchor=tk.W)\n",
    "    tree.heading(\"comment\", text=\"Comentarios Originales\", anchor=tk.W)\n",
    "    tree.heading(\"comment_limpio\", text=\"Comentarios Limpios\", anchor=tk.W)\n",
    "    tree.heading(\"tokens\", text=\"Tokens\", anchor=tk.W)\n",
    "    tree.heading(\"tfidf\", text=\"TF-IDF\", anchor=tk.W)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        tfidf_vector = tfidf_matrix[index].toarray().flatten()\n",
    "        tfidf_scores = {feature_names[i]: tfidf_vector[i] for i in range(len(feature_names)) if tfidf_vector[i] > 0}\n",
    "        tree.insert(\"\", index, text=\"\", values=(row[\"comment\"], row[\"comment_limpio\"], row[\"tokens\"], str(tfidf_scores)))\n",
    "\n",
    "    tree.pack(expand=True, fill='both')\n",
    "    \n",
    "    root.mainloop()\n",
    "\n",
    "def generar_visualizaciones(df, y_test, y_pred_classes, clases, cm):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(15, 15))\n",
    "    fig.suptitle('Análisis de Sentimientos y Aspectos')\n",
    "\n",
    "    # Distribución de Sentimientos\n",
    "    sns.countplot(ax=axes[0, 0], x=df['aspecto'], order=['positivo', 'negativo', 'neutral'])\n",
    "    axes[0, 0].set_title('Distribución de Sentimientos')\n",
    "    axes[0, 0].set_xlabel('Sentimiento')\n",
    "    axes[0, 0].set_ylabel('Conteo')\n",
    "\n",
    "    # Número de Comentarios por Año\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['fecha'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "        df.set_index('fecha', inplace=True)\n",
    "        df.resample('Y').size().plot(ax=axes[0, 1])\n",
    "        axes[0, 1].set_title('Número de Comentarios por Año')\n",
    "        axes[0, 1].set_xlabel('Fecha')\n",
    "        axes[0, 1].set_ylabel('Número de Comentarios')\n",
    "    else:\n",
    "        axes[0, 1].set_title('Número de Comentarios por Año (No disponible)')\n",
    "        axes[0, 1].set_xlabel('Fecha')\n",
    "        axes[0, 1].set_ylabel('Número de Comentarios')\n",
    "\n",
    "    # Distribución de Aspectos\n",
    "    sns.countplot(ax=axes[1, 0], x=df['aspecto'], order=['positivo', 'negativo', 'neutral'])\n",
    "    axes[1, 0].set_title('Distribución de Aspectos')\n",
    "    axes[1, 0].set_xlabel('Aspectos')\n",
    "    axes[1, 0].set_ylabel('Conteo')\n",
    "\n",
    "    # Palabras más frecuentes\n",
    "    from collections import Counter\n",
    "    all_words = ' '.join(df['comment_limpio']).split()\n",
    "    word_freq = Counter(all_words)\n",
    "    common_words = word_freq.most_common(20)\n",
    "    words, counts = zip(*common_words)\n",
    "    axes[1, 1].pie(counts, labels=words, autopct='%1.1f%%')\n",
    "    axes[1, 1].set_title('Palabras Más Frecuentes')\n",
    "\n",
    "    # Distribución de Opiniones por Año\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['año'] = pd.to_datetime(df['timestamp']).dt.year\n",
    "        sns.countplot(ax=axes[2, 0], data=df, x='año', hue='aspecto', order=sorted(df['año'].unique()))\n",
    "        axes[2, 0].set_title('Distribución de Opiniones por Año')\n",
    "        axes[2, 0].set_xlabel('Año')\n",
    "        axes[2, 0].set_ylabel('Conteo')\n",
    "    else:\n",
    "        axes[2, 0].set_title('Distribución de Opiniones por Año (No disponible)')\n",
    "        axes[2, 0].set_xlabel('Año')\n",
    "        axes[2, 0].set_ylabel('Conteo')\n",
    "\n",
    "    # Matriz de Confusión\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[2, 1], xticklabels=clases, yticklabels=clases)\n",
    "    axes[2, 1].set_title('Confusion Matrix')\n",
    "    axes[2, 1].set_xlabel('Predicted')\n",
    "    axes[2, 1].set_ylabel('True')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    \n",
    "    # Guardar la figura completa\n",
    "    fig.savefig('analisis_sentimientos_aspectos.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1b95e5",
   "metadata": {},
   "source": [
    "# SECCIÓN 4: EJECUCIÓN PRINCIPAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "559f8244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos...\n",
      "Datos cargados: 2878 registros, 3 columnas\n",
      "           username                 timestamp  \\\n",
      "0      @MarcoGilerM  2023-09-11T13:37:46.000Z   \n",
      "1  @JoseAnt40060050  2023-09-01T22:39:33.000Z   \n",
      "2  @MariaMa99904828  2023-05-31T00:22:19.000Z   \n",
      "3      @patsurfer71  2023-03-30T12:27:34.000Z   \n",
      "4  @Joselui48173725  2023-02-01T13:43:37.000Z   \n",
      "5    @juancasaenz76  2023-01-31T18:21:38.000Z   \n",
      "6     @radialvision  2023-01-30T20:57:29.000Z   \n",
      "7    @mmendozabelen  2023-01-30T19:11:53.000Z   \n",
      "8         @HRZManta  2022-07-08T20:08:41.000Z   \n",
      "9         @HRZManta  2022-06-13T16:33:54.000Z   \n",
      "\n",
      "                                             comment  \n",
      "0  eso no es todo al realizarse la transferencia ...  \n",
      "1  nunca entregaste un hospital para la zona sur ...  \n",
      "2  a mas de adefesio sinverguenza charlatan cuand...  \n",
      "3  jipijapa necesita un hospital general que pued...  \n",
      "4  hay jente que tienen caca en el serebropor k v...  \n",
      "5  x ej los dos funcionarios atacados a bala en p...  \n",
      "6  noticias  se trataria de la jueza genny guanol...  \n",
      "7  se trataria de la jueza genny guanoluisa delga...  \n",
      "8  este tipo de cirugias se realizan de forma pro...  \n",
      "9  esta unidad mantiene 1096 casos vinculados des...  \n",
      "Limpiando datos...\n",
      "Columnas disponibles después de limpiar los datos: Index(['username', 'timestamp', 'comment', 'comment_limpio', 'tokens'], dtype='object')\n",
      "           username                 timestamp  \\\n",
      "0      @MarcoGilerM  2023-09-11T13:37:46.000Z   \n",
      "1  @JoseAnt40060050  2023-09-01T22:39:33.000Z   \n",
      "2  @MariaMa99904828  2023-05-31T00:22:19.000Z   \n",
      "3      @patsurfer71  2023-03-30T12:27:34.000Z   \n",
      "4  @Joselui48173725  2023-02-01T13:43:37.000Z   \n",
      "5    @juancasaenz76  2023-01-31T18:21:38.000Z   \n",
      "6     @radialvision  2023-01-30T20:57:29.000Z   \n",
      "7    @mmendozabelen  2023-01-30T19:11:53.000Z   \n",
      "8         @HRZManta  2022-07-08T20:08:41.000Z   \n",
      "9         @HRZManta  2022-06-13T16:33:54.000Z   \n",
      "\n",
      "                                             comment  \\\n",
      "0  eso no es todo al realizarse la transferencia ...   \n",
      "1  nunca entregaste un hospital para la zona sur ...   \n",
      "2  a mas de adefesio sinverguenza charlatan cuand...   \n",
      "3  jipijapa necesita un hospital general que pued...   \n",
      "4  hay jente que tienen caca en el serebropor k v...   \n",
      "5  x ej los dos funcionarios atacados a bala en p...   \n",
      "6  noticias  se trataria de la jueza genny guanol...   \n",
      "7  se trataria de la jueza genny guanoluisa delga...   \n",
      "8  este tipo de cirugias se realizan de forma pro...   \n",
      "9  esta unidad mantiene 1096 casos vinculados des...   \n",
      "\n",
      "                                      comment_limpio  \\\n",
      "0  realizarse transferencia paciente herido manta...   \n",
      "1  nunca entregaste hospital zona sur manabi jipi...   \n",
      "2  ma adefesio sinverguenza charlatan visito bahi...   \n",
      "3  jipijapa necesita hospital general pueda dar a...   \n",
      "4  jente caca serebropor k viven agua lu cola noc...   \n",
      "5  x ej do funcionarios atacados bala pajan estan...   \n",
      "6  noticias trataria jueza genny guanoluisa delga...   \n",
      "7  trataria jueza genny guanoluisa delgado secret...   \n",
      "8  tipo cirugias realizan forma programada poblac...   \n",
      "9  unidad mantiene casos vinculados inicios impor...   \n",
      "\n",
      "                                              tokens  \n",
      "0  [realizarse, transferencia, paciente, herido, ...  \n",
      "1  [nunca, entregaste, hospital, zona, sur, manab...  \n",
      "2  [ma, adefesio, sinverguenza, charlatan, visito...  \n",
      "3  [jipijapa, necesita, hospital, general, pueda,...  \n",
      "4  [jente, caca, serebropor, k, viven, agua, lu, ...  \n",
      "5  [x, ej, do, funcionarios, atacados, bala, paja...  \n",
      "6  [noticias, trataria, jueza, genny, guanoluisa,...  \n",
      "7  [trataria, jueza, genny, guanoluisa, delgado, ...  \n",
      "8  [tipo, cirugias, realizan, forma, programada, ...  \n",
      "9  [unidad, mantiene, casos, vinculados, inicios,...  \n",
      "Vectorizando texto...\n",
      "Mostrando resultados de limpieza de datos...\n",
      "Análisis de sentimientos...\n",
      "Columnas disponibles después del análisis de sentimientos: Index(['username', 'timestamp', 'comment', 'comment_limpio', 'tokens',\n",
      "       'aspecto', 'sentimiento'],\n",
      "      dtype='object')\n",
      "Emociones detectadas: {'positivo': 206, 'negativo': 103, 'neutral': 2569}\n",
      "           username                 timestamp  \\\n",
      "0      @MarcoGilerM  2023-09-11T13:37:46.000Z   \n",
      "1  @JoseAnt40060050  2023-09-01T22:39:33.000Z   \n",
      "2  @MariaMa99904828  2023-05-31T00:22:19.000Z   \n",
      "3      @patsurfer71  2023-03-30T12:27:34.000Z   \n",
      "4  @Joselui48173725  2023-02-01T13:43:37.000Z   \n",
      "5    @juancasaenz76  2023-01-31T18:21:38.000Z   \n",
      "6     @radialvision  2023-01-30T20:57:29.000Z   \n",
      "7    @mmendozabelen  2023-01-30T19:11:53.000Z   \n",
      "8         @HRZManta  2022-07-08T20:08:41.000Z   \n",
      "9         @HRZManta  2022-06-13T16:33:54.000Z   \n",
      "\n",
      "                                             comment  \\\n",
      "0  eso no es todo al realizarse la transferencia ...   \n",
      "1  nunca entregaste un hospital para la zona sur ...   \n",
      "2  a mas de adefesio sinverguenza charlatan cuand...   \n",
      "3  jipijapa necesita un hospital general que pued...   \n",
      "4  hay jente que tienen caca en el serebropor k v...   \n",
      "5  x ej los dos funcionarios atacados a bala en p...   \n",
      "6  noticias  se trataria de la jueza genny guanol...   \n",
      "7  se trataria de la jueza genny guanoluisa delga...   \n",
      "8  este tipo de cirugias se realizan de forma pro...   \n",
      "9  esta unidad mantiene 1096 casos vinculados des...   \n",
      "\n",
      "                                      comment_limpio  \\\n",
      "0  realizarse transferencia paciente herido manta...   \n",
      "1  nunca entregaste hospital zona sur manabi jipi...   \n",
      "2  ma adefesio sinverguenza charlatan visito bahi...   \n",
      "3  jipijapa necesita hospital general pueda dar a...   \n",
      "4  jente caca serebropor k viven agua lu cola noc...   \n",
      "5  x ej do funcionarios atacados bala pajan estan...   \n",
      "6  noticias trataria jueza genny guanoluisa delga...   \n",
      "7  trataria jueza genny guanoluisa delgado secret...   \n",
      "8  tipo cirugias realizan forma programada poblac...   \n",
      "9  unidad mantiene casos vinculados inicios impor...   \n",
      "\n",
      "                                              tokens  aspecto  sentimiento  \n",
      "0  [realizarse, transferencia, paciente, herido, ...  neutral            2  \n",
      "1  [nunca, entregaste, hospital, zona, sur, manab...  neutral            2  \n",
      "2  [ma, adefesio, sinverguenza, charlatan, visito...  neutral            2  \n",
      "3  [jipijapa, necesita, hospital, general, pueda,...  neutral            2  \n",
      "4  [jente, caca, serebropor, k, viven, agua, lu, ...  neutral            2  \n",
      "5  [x, ej, do, funcionarios, atacados, bala, paja...  neutral            2  \n",
      "6  [noticias, trataria, jueza, genny, guanoluisa,...  neutral            2  \n",
      "7  [trataria, jueza, genny, guanoluisa, delgado, ...  neutral            2  \n",
      "8  [tipo, cirugias, realizan, forma, programada, ...  neutral            2  \n",
      "9  [unidad, mantiene, casos, vinculados, inicios,...  neutral            2  \n",
      "Contenido de 'sentimiento_label':\n",
      "[2 0 1]\n",
      "Valores NaN en 'sentimiento': 0\n",
      "Datos después de eliminar NaN en 'sentimiento': 2878 registros\n",
      "           username                 timestamp  \\\n",
      "0      @MarcoGilerM  2023-09-11T13:37:46.000Z   \n",
      "1  @JoseAnt40060050  2023-09-01T22:39:33.000Z   \n",
      "2  @MariaMa99904828  2023-05-31T00:22:19.000Z   \n",
      "3      @patsurfer71  2023-03-30T12:27:34.000Z   \n",
      "4  @Joselui48173725  2023-02-01T13:43:37.000Z   \n",
      "5    @juancasaenz76  2023-01-31T18:21:38.000Z   \n",
      "6     @radialvision  2023-01-30T20:57:29.000Z   \n",
      "7    @mmendozabelen  2023-01-30T19:11:53.000Z   \n",
      "8         @HRZManta  2022-07-08T20:08:41.000Z   \n",
      "9         @HRZManta  2022-06-13T16:33:54.000Z   \n",
      "\n",
      "                                             comment  \\\n",
      "0  eso no es todo al realizarse la transferencia ...   \n",
      "1  nunca entregaste un hospital para la zona sur ...   \n",
      "2  a mas de adefesio sinverguenza charlatan cuand...   \n",
      "3  jipijapa necesita un hospital general que pued...   \n",
      "4  hay jente que tienen caca en el serebropor k v...   \n",
      "5  x ej los dos funcionarios atacados a bala en p...   \n",
      "6  noticias  se trataria de la jueza genny guanol...   \n",
      "7  se trataria de la jueza genny guanoluisa delga...   \n",
      "8  este tipo de cirugias se realizan de forma pro...   \n",
      "9  esta unidad mantiene 1096 casos vinculados des...   \n",
      "\n",
      "                                      comment_limpio  \\\n",
      "0  realizarse transferencia paciente herido manta...   \n",
      "1  nunca entregaste hospital zona sur manabi jipi...   \n",
      "2  ma adefesio sinverguenza charlatan visito bahi...   \n",
      "3  jipijapa necesita hospital general pueda dar a...   \n",
      "4  jente caca serebropor k viven agua lu cola noc...   \n",
      "5  x ej do funcionarios atacados bala pajan estan...   \n",
      "6  noticias trataria jueza genny guanoluisa delga...   \n",
      "7  trataria jueza genny guanoluisa delgado secret...   \n",
      "8  tipo cirugias realizan forma programada poblac...   \n",
      "9  unidad mantiene casos vinculados inicios impor...   \n",
      "\n",
      "                                              tokens  aspecto  sentimiento  \n",
      "0  [realizarse, transferencia, paciente, herido, ...  neutral            2  \n",
      "1  [nunca, entregaste, hospital, zona, sur, manab...  neutral            2  \n",
      "2  [ma, adefesio, sinverguenza, charlatan, visito...  neutral            2  \n",
      "3  [jipijapa, necesita, hospital, general, pueda,...  neutral            2  \n",
      "4  [jente, caca, serebropor, k, viven, agua, lu, ...  neutral            2  \n",
      "5  [x, ej, do, funcionarios, atacados, bala, paja...  neutral            2  \n",
      "6  [noticias, trataria, jueza, genny, guanoluisa,...  neutral            2  \n",
      "7  [trataria, jueza, genny, guanoluisa, delgado, ...  neutral            2  \n",
      "8  [tipo, cirugias, realizan, forma, programada, ...  neutral            2  \n",
      "9  [unidad, mantiene, casos, vinculados, inicios,...  neutral            2  \n",
      "Dividiendo datos en entrenamiento, validación y prueba...\n",
      "Datos de entrenamiento: 2014 registros\n",
      "Datos de validación: 288 registros\n",
      "Datos de prueba: 576 registros\n",
      "              username                 timestamp  \\\n",
      "471   @jhonnyjimenezmr  2023-09-04T12:26:04.000Z   \n",
      "1453      @GoberManabi  2023-03-24T21:20:43.000Z   \n",
      "2376          @andod11  2015-04-22T13:23:18.000Z   \n",
      "1601        @eloysorsa  2020-06-24T03:34:12.000Z   \n",
      "1094         @sergia08  2023-01-08T22:02:30.000Z   \n",
      "1255    @DiarioExtraEc  2019-07-12T16:07:38.000Z   \n",
      "1128        @GKecuador  2022-11-30T16:30:00.000Z   \n",
      "2251     @Rocafuerte18  2020-07-23T03:02:43.000Z   \n",
      "945         @Gabopazec  2024-02-21T22:52:59.000Z   \n",
      "1800  @NeuroDrMartinez  2023-12-01T01:54:16.000Z   \n",
      "\n",
      "                                                comment  \\\n",
      "471   si porque ni con el dinero que se recaudo como...   \n",
      "1453  el gobernador  hernan0722  se reunio con el al...   \n",
      "2376  mauriciorodasec  sr alcalde ayuda urgente con ...   \n",
      "1601  a jipijapa venistes a inaugurar una casa una c...   \n",
      "1094  pido ayuda una joven q vi sufrir crecer y supe...   \n",
      "1255  atencion un defensor publico de el canton el c...   \n",
      "1128  los sicarios intentaban matar a un joven de 16...   \n",
      "2251  y porque no mandaron a rosario robles al hospi...   \n",
      "945   una imagen que parte el alma un bebe es sacado...   \n",
      "1800  game over 110 anos de neurocirugia en el hospi...   \n",
      "\n",
      "                                         comment_limpio  \\\n",
      "471   si dinero recaudo parte terremoto iva pudo rec...   \n",
      "1453  gobernador hernan reunio alcalde jipijapa luis...   \n",
      "2376  mauriciorodasec sr alcalde ayuda urgente adoqu...   \n",
      "1601  jipijapa venistes inaugurar casa casa casa sac...   \n",
      "1094  pido ayuda joven q vi sufrir crecer superarse ...   \n",
      "1255  atencion defensor publico canton carmen manabi...   \n",
      "1128  sicarios intentaban matar joven ano siendo ate...   \n",
      "2251            mandaron rosario roble hospital tambien   \n",
      "945   imagen parte alma bebe sacado termocuna hospit...   \n",
      "1800  game over ano neurocirugia hospital luis verna...   \n",
      "\n",
      "                                                 tokens  aspecto  sentimiento  \n",
      "471   [si, dinero, recaudo, parte, terremoto, iva, p...  neutral            2  \n",
      "1453  [gobernador, hernan, reunio, alcalde, jipijapa...  neutral            2  \n",
      "2376  [mauriciorodasec, sr, alcalde, ayuda, urgente,...  neutral            2  \n",
      "1601  [jipijapa, venistes, inaugurar, casa, casa, ca...  neutral            2  \n",
      "1094  [pido, ayuda, joven, q, vi, sufrir, crecer, su...  neutral            2  \n",
      "1255  [atencion, defensor, publico, canton, carmen, ...  neutral            2  \n",
      "1128  [sicarios, intentaban, matar, joven, ano, sien...  neutral            2  \n",
      "2251      [mandaron, rosario, roble, hospital, tambien]  neutral            2  \n",
      "945   [imagen, parte, alma, bebe, sacado, termocuna,...  neutral            2  \n",
      "1800  [game, over, ano, neurocirugia, hospital, luis...  neutral            2  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Erick Carreño\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo cargado exitosamente.\n",
      "Evaluando el modelo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Cargar datos\n",
    "print(\"Cargando datos...\")\n",
    "df = pd.read_csv('datasets/datos_combinados_1.csv')\n",
    "print(f\"Datos cargados: {len(df)} registros, {len(df.columns)} columnas\")\n",
    "print(df.head(10))\n",
    "\n",
    "# Limpiar datos\n",
    "print(\"Limpiando datos...\")\n",
    "df = limpiar_datos(df)\n",
    "print(f\"Columnas disponibles después de limpiar los datos: {df.columns}\")\n",
    "print(df.head(10))\n",
    "\n",
    "# Vectorizar texto y mostrar resultados\n",
    "print(\"Vectorizando texto...\")\n",
    "tfidf_matrix, feature_names = vectorizar_texto(df)\n",
    "print(\"Mostrando resultados de limpieza de datos...\")\n",
    "mostrar_resultados(df, tfidf_matrix, feature_names)\n",
    "\n",
    "# Análisis de sentimientos\n",
    "print(\"Análisis de sentimientos...\")\n",
    "df, emociones = analizar_sentimientos(df)\n",
    "print(f\"Columnas disponibles después del análisis de sentimientos: {df.columns}\")\n",
    "print(f\"Emociones detectadas: { {key: len(value) for key, value in emociones.items()} }\")\n",
    "print(df.head(10))\n",
    "\n",
    "# Verificar contenido de 'sentimiento_label'\n",
    "print(\"Contenido de 'sentimiento_label':\")\n",
    "print(df['sentimiento'].unique())\n",
    "\n",
    "# Verificar valores NaN antes de eliminar\n",
    "print(f\"Valores NaN en 'sentimiento': {df['sentimiento'].isna().sum()}\")\n",
    "\n",
    "# Manejar valores NaN\n",
    "df = df.dropna(subset=['sentimiento', 'comment_limpio'])\n",
    "\n",
    "# Verificar DataFrame después de eliminar valores NaN\n",
    "print(f\"Datos después de eliminar NaN en 'sentimiento': {len(df)} registros\")\n",
    "print(df.head(10))\n",
    "\n",
    "# Dividir datos en entrenamiento, validación y prueba\n",
    "print(\"Dividiendo datos en entrenamiento, validación y prueba...\")\n",
    "train_df, val_df, test_df = np.split(df.sample(frac=1, random_state=42), [int(.7*len(df)), int(.8*len(df))])\n",
    "print(f\"Datos de entrenamiento: {len(train_df)} registros\")\n",
    "print(f\"Datos de validación: {len(val_df)} registros\")\n",
    "print(f\"Datos de prueba: {len(test_df)} registros\")\n",
    "print(train_df.head(10))\n",
    "\n",
    "# Verificar datos de entrenamiento\n",
    "if len(train_df) == 0:\n",
    "    raise ValueError(\"Datos de entrenamiento están vacíos.\")\n",
    "\n",
    "# Tokenizar los datos\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=5000)  # Reducir el vocabulario a 5000 palabras\n",
    "tokenizer.fit_on_texts(train_df['comment_limpio'].values)\n",
    "X_train = tokenizer.texts_to_sequences(train_df['comment_limpio'].values)\n",
    "X_val = tokenizer.texts_to_sequences(val_df['comment_limpio'].values)\n",
    "X_test = tokenizer.texts_to_sequences(test_df['comment_limpio'].values)  # Tokenizar datos de prueba\n",
    "\n",
    "# Padding\n",
    "max_len = 50  # Reducir el maxlen a 50\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "X_val = tf.keras.preprocessing.sequence.pad_sequences(X_val, maxlen=max_len)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_len)  # Padding de datos de prueba\n",
    "\n",
    "# Convertir etiquetas a numpy arrays\n",
    "y_train = train_df['sentimiento'].values\n",
    "y_val = val_df['sentimiento'].values\n",
    "y_test = test_df['sentimiento'].values  # Etiquetas de prueba\n",
    "\n",
    "# Cargar o entrenar el modelo\n",
    "model, loaded_tokenizer = cargar_modelo_rnn()\n",
    "if model is None or loaded_tokenizer is None:\n",
    "    # Entrenar el modelo, guardar el modelo y el tokenizer\n",
    "    print(\"Entrenando el modelo...\")\n",
    "    model = definir_modelo_rnn(vocab_size=5000, max_len=max_len)\n",
    "    history = entrenar_y_guardar_modelo_rnn(model, X_train, y_train, X_val, y_val, tokenizer, epochs=50, batch_size=32)\n",
    "else:\n",
    "    print(\"Modelo cargado exitosamente.\")\n",
    "    tokenizer = loaded_tokenizer\n",
    "    history = None  # No habrá historia si cargamos un modelo preexistente\n",
    "\n",
    "# Evaluar el modelo\n",
    "print(\"Evaluando el modelo...\")\n",
    "cr, cm = evaluar_modelo_rnn(model, test_df, tokenizer)\n",
    "\n",
    "# Generar visualizaciones\n",
    "print(\"Generando visualizaciones...\")\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "generar_visualizaciones(df, y_test, y_pred_classes, ['positivo', 'negativo', 'neutral'], cm)\n",
    "print(\"Visualizaciones generadas.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
